{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW6B6ZQ7gh5kHjRS7VVb9G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karankulshrestha/ai-notebooks/blob/main/regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization\n",
        "It is refer as a technique to reduce the algorithm overfitting by introducing some penalty or constraints and make it generalized on unseen data, even if the training error increases."
      ],
      "metadata": {
        "id": "g9JkUL2dQJfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L2 Regularization (Ridge)\n",
        "L2 Regularization helps prevent overfitting by keeping the modelâ€™s weights small. Large weights make the model too sensitive to training data, causing overfitting. To control this, we add a penalty term (the sum of the squares of all weights) to the loss function â€” for example, along with Mean Squared Error (MSE).\n",
        "\n",
        "During backpropagation, this penalty discourages large weights, gradually shrinking them and resulting in a smoother, more generalized model.\n",
        "\n",
        "It works in 90% cases"
      ],
      "metadata": {
        "id": "Jf8J5q5nRrSU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUQzfAa3Ne-7",
        "outputId": "091be4b3-d557-4459-a88d-fc50a2e1352f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We add a panelty { lambda_red * (w**2) } to the loss 19.4\n",
            "\n",
            "The penalty makes the loss bigger when weights are large. Because loss is bigger, the gradient update becomes stronger for those large weights.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# sample data\n",
        "X = np.array([[1], [2], [3], [4]])\n",
        "y = np.array([2, 4, 6, 8])\n",
        "\n",
        "# initialize the weight\n",
        "w = 0.5\n",
        "lambda_red = 0.1 # Regularization strength\n",
        "\n",
        "y_pred = X * w\n",
        "\n",
        "# loss\n",
        "mse_loss = np.mean((y - y_pred)**2)\n",
        "\n",
        "l2_loss = mse_loss + lambda_red * (w**2)\n",
        "\n",
        "print(\"We add a panelty { lambda_red * (w**2) } to the loss\", l2_loss)\n",
        "\n",
        "print(\"\\nThe penalty makes the loss bigger when weights are large. Because loss is bigger, the gradient update becomes stronger for those large weights.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L1 Regularization (Lasso)\n",
        "L1 regularization adds a penalty for large weights using their absolute values (|w|).\n",
        "It makes the model simpler by forcing some weights to become zero,\n",
        "which helps prevent overfitting and select only the most important features.\n",
        "\n",
        "If your normal loss is **Mean Squared Error (MSE):**\n",
        "\n",
        "$$\n",
        "\\text{Loss} = \\text{MSE}\n",
        "$$\n",
        "\n",
        "Then with **L1 regularization**, it becomes:\n",
        "\n",
        "$$\n",
        "\\text{Loss} = \\text{MSE} + \\lambda \\sum |w|\n",
        "$$\n",
        "\n",
        "where  \n",
        "- \\(Î» lambda \\) = regularization strength  \n",
        "- \\( w \\) = model weights"
      ],
      "metadata": {
        "id": "juXgPUh1WiUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4]])\n",
        "y = np.array([2, 4, 6, 8])\n",
        "\n",
        "# Initialize a weight\n",
        "w = 0.5\n",
        "lambda_reg = 0.1  # Regularization strength\n",
        "\n",
        "# Predicted values\n",
        "y_pred = X * w\n",
        "\n",
        "# Mean Squared Error (MSE)\n",
        "mse_loss = np.mean((y - y_pred) ** 2)\n",
        "\n",
        "# L1 Regularization term (sum of absolute weights)\n",
        "l1_penalty = lambda_reg * np.abs(w)\n",
        "\n",
        "# Total loss = MSE + L1 penalty\n",
        "l1_loss = mse_loss + l1_penalty\n",
        "\n",
        "print(\"MSE Loss:\", mse_loss)\n",
        "print(\"L1 Penalty:\", l1_penalty)\n",
        "print(\"Total Loss with L1 Regularization:\", l1_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLr-o8_4ULDH",
        "outputId": "0742f400-4c3e-401f-b7b3-d499be354bce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE Loss: 19.375\n",
            "L1 Penalty: 0.05\n",
            "Total Loss with L1 Regularization: 19.425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Augmentation\n",
        "Dataset augmentation is like taking your current data and making new versions of it by flipping, rotating, cropping, adding noise, or slightly changing it,\n",
        "so your model learns to recognize patterns better and doesnâ€™t overfit.\n",
        "\n",
        "Itâ€™s mostly used in machine learning and deep learning, especially for images, audio, and text."
      ],
      "metadata": {
        "id": "ltiwSS5Tbk-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define a set of random augmentations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.RandomResizedCrop(224)\n",
        "])\n",
        "\n",
        "# Apply this to your training dataset\n",
        "# train_dataset = torchvision.datasets.ImageFolder(\"data/train\", transform=transform)"
      ],
      "metadata": {
        "id": "g3IL7kwZYGRm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout\n",
        "Hidden layer outputs: [0.3, 0.5, 0.8, 0.2]\n",
        "\n",
        "If dropout = 0.5 (50% dropout rate)\n",
        "\n",
        "After dropout: [0.3, 0, 0.8, 0]\n"
      ],
      "metadata": {
        "id": "GU1LVlUumShf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Normalization - The Optimization Plus\n",
        "**What It Does:** It makes sure that the data flowing through the network doesnâ€™t get too big or too small after each layer (pre-activation)\n",
        "\n",
        "**Primary Purpose:** The values inside can explode (become very large) or vanish (become very small) after many layers.\n",
        "\n",
        "**Bonus Effect:** Acts as regularization (sometimes makes dropout unnecessary)\n",
        "\n",
        "**Modern Reality:** Very popular in deep learning"
      ],
      "metadata": {
        "id": "hjXH2Jhjmuav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ§® Batch Normalization Formula\n",
        "$$\n",
        "\\mathrm{BN}(x) = \\gamma \\left( \\frac{x - \\mu}{\\sqrt{\\sigma^{2} + \\epsilon}} \\right) + \\beta\n",
        "$$\n",
        "**Where:**\n",
        "- $x$ = neuron activation  \n",
        "- $\\mu$ = mean of activations in the batch  \n",
        "- $\\sigma^{2}$ = variance of activations in the batch  \n",
        "- $\\epsilon$ = small number (to avoid division by zero)  \n",
        "- $\\gamma, \\beta$ = learned scale and shift parameters"
      ],
      "metadata": {
        "id": "KjJO5RJWnpL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Normalization\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example for a fully connected network\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 64),\n",
        "    nn.BatchNorm1d(64),  # Batch Normalization for 1D input\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 1)\n",
        ")"
      ],
      "metadata": {
        "id": "S0oVjZJ0gWi2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Training\n",
        "Adversarial Training means we train a model not just on normal data,\n",
        "but also on tricky, slightly modified data that tries to fool the model."
      ],
      "metadata": {
        "id": "6re_zbAOwIAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    # Add small noise in the direction that increases the loss\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    perturbed_image = image + epsilon * sign_data_grad\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)  # keep pixel values valid\n",
        "    return perturbed_image"
      ],
      "metadata": {
        "id": "_R_5WqHUoQt4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tcck2dZjwUEg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}