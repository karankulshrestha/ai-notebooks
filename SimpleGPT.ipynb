{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+nIRvi9fWLhkqljbJB80L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karankulshrestha/ai-notebooks/blob/main/SimpleGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_QQIMGL3ySc",
        "outputId": "071d92a0-9aac-4c77-f7c0-695ddd5686bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-23 14:47:52--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.4’\n",
            "\n",
            "\rinput.txt.4           0%[                    ]       0  --.-KB/s               \rinput.txt.4         100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-06-23 14:47:52 (129 MB/s) - ‘input.txt.4’ saved [1115394/1115394]\n",
            "\n",
            "0.209729 M parameters\n",
            "step 0: train loss 4.4111, val loss 4.4018\n",
            "step 100: train loss 2.6551, val loss 2.6645\n",
            "step 200: train loss 2.5084, val loss 2.5036\n",
            "step 300: train loss 2.4155, val loss 2.4296\n",
            "step 400: train loss 2.3477, val loss 2.3539\n",
            "step 500: train loss 2.2966, val loss 2.3118\n",
            "step 600: train loss 2.2411, val loss 2.2500\n",
            "step 700: train loss 2.2019, val loss 2.2157\n",
            "step 800: train loss 2.1635, val loss 2.1855\n",
            "step 900: train loss 2.1237, val loss 2.1495\n",
            "step 1000: train loss 2.0988, val loss 2.1246\n",
            "step 1100: train loss 2.0683, val loss 2.1143\n",
            "step 1200: train loss 2.0332, val loss 2.0744\n",
            "step 1300: train loss 2.0261, val loss 2.0631\n",
            "step 1400: train loss 1.9909, val loss 2.0367\n",
            "step 1500: train loss 1.9668, val loss 2.0264\n",
            "step 1600: train loss 1.9520, val loss 2.0313\n",
            "step 1700: train loss 1.9431, val loss 2.0098\n",
            "step 1800: train loss 1.9112, val loss 1.9968\n",
            "step 1900: train loss 1.9029, val loss 1.9760\n",
            "step 2000: train loss 1.8813, val loss 1.9836\n",
            "step 2100: train loss 1.8704, val loss 1.9706\n",
            "step 2200: train loss 1.8513, val loss 1.9544\n",
            "step 2300: train loss 1.8484, val loss 1.9487\n",
            "step 2400: train loss 1.8415, val loss 1.9383\n",
            "step 2500: train loss 1.8117, val loss 1.9325\n",
            "step 2600: train loss 1.8219, val loss 1.9359\n",
            "step 2700: train loss 1.8057, val loss 1.9253\n",
            "step 2800: train loss 1.7975, val loss 1.9202\n",
            "step 2900: train loss 1.7947, val loss 1.9203\n",
            "step 3000: train loss 1.7945, val loss 1.9113\n",
            "step 3100: train loss 1.7661, val loss 1.9123\n",
            "step 3200: train loss 1.7524, val loss 1.9055\n",
            "step 3300: train loss 1.7597, val loss 1.9003\n",
            "step 3400: train loss 1.7517, val loss 1.8862\n",
            "step 3500: train loss 1.7381, val loss 1.8847\n",
            "step 3600: train loss 1.7230, val loss 1.8827\n",
            "step 3700: train loss 1.7263, val loss 1.8767\n",
            "step 3800: train loss 1.7167, val loss 1.8824\n",
            "step 3900: train loss 1.7137, val loss 1.8604\n",
            "step 4000: train loss 1.7018, val loss 1.8456\n",
            "step 4100: train loss 1.7072, val loss 1.8719\n",
            "step 4200: train loss 1.7039, val loss 1.8560\n",
            "step 4300: train loss 1.6976, val loss 1.8344\n",
            "step 4400: train loss 1.7002, val loss 1.8492\n",
            "step 4500: train loss 1.6876, val loss 1.8478\n",
            "step 4600: train loss 1.6858, val loss 1.8346\n",
            "step 4700: train loss 1.6752, val loss 1.8261\n",
            "step 4800: train loss 1.6644, val loss 1.8342\n",
            "step 4900: train loss 1.6616, val loss 1.8292\n",
            "step 4999: train loss 1.6621, val loss 1.8152\n",
            "\n",
            "And they brince.\n",
            "\n",
            "Shall To Richands:\n",
            "Thou but take Onder that hath barnit he us hath bury.\n",
            "\n",
            "XENoway, arch, my feanst,--my In heavens,\n",
            "Hoffisted me my would bures if ensen cinilatisely;\n",
            "Whits, and the now on you muself: no me little;\n",
            "Hongry you are nees him youngring\n",
            "In am patelives not\n",
            "that demed again Willon her evily, where dothing with in his son\n",
            "of his guars and thrust for trears to shall eyes,\n",
            "Unctation Pried my offer-from and tood is:\n",
            "Sads bether grest huth courrear tey it-broth\n",
            "And for the neever, I marry how I was my me\n",
            "That hense mennorablend them, and whom the head and us.\n",
            "\n",
            "ISATHAS\n",
            "Stry speaks to cas lord\n",
            "In as dettureings.\n",
            "\n",
            "LUCIO:\n",
            "Abelenor to much some of's crupeed; buse so upon sure feed talk:\n",
            "Whith I shecest is gone duke in,\n",
            "To may Greant beaken agave thungs all. I wife, may monder so lack.\n",
            "\n",
            "Prive, I'll honoury?\n",
            "Their have brother, do this mister,\n",
            "Hold is onter not the summe will;\n",
            "Gron whon than the must this bell madon, and therefore mewatch time wout\n",
            "Shank'd;\n",
            "To know our her and bruty hears, If\n",
            "twonguis out Mustentine, her not\n",
            "take all not wee or word.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Where, bying your sove, gret, for thy day\n",
            "An bearen\n",
            "Hear wish I have a canies and love be and inkely,\n",
            "To me attern true exreast men is whill-geat toge\n",
            "But dedetiony's lasters too usle.\n",
            "\n",
            "KING RICHARD II:\n",
            "Najes' the vown;\n",
            "Maurth?\n",
            "\n",
            "Second Now, he is is stille.\n",
            "\n",
            "POMPEding'dlame,\n",
            "Yet musts been smeen not bust they gain the bae then where him.\n",
            "\n",
            "ROMEO:\n",
            "I will tenge dele, get instried:\n",
            "And arrow ulliederer, smen toke thou grapty;\n",
            "And, every freed thou a-lates crres to me\n",
            "been when seeks with that breed accurieved him kLet in why him.\n",
            "\n",
            "LADY RANCE:\n",
            "And wound arwill-shrit, give affetch,\n",
            "Toh, them as though, God take twake\n",
            "And then, the wifly this father, Edwry,--\n",
            "\n",
            "JULIET:\n",
            "No, liveory, take weak the sent.\n",
            "\n",
            "HENRY BOPILLA:\n",
            "Why, I'll neizy is.\n",
            "Why, I unswerer how to scause; whom good not your your son!\n",
            "No, I wase trong what welcom though ston parforit briff thy not fight his suppect,\n",
            "She being of\n",
            "all \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameter\n",
        "batch_size = 16 # how many sentences at once want to process.\n",
        "block_size = 32 # number of words our each sentence contains.\n",
        "max_iters = 5000 # number of times, we train the model\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3 # learning rate is used to scale and shift the parameters\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200 # number of times we test our model\n",
        "n_embed = 64 # number of features our each word of the sentence have, in this case 64 features.\n",
        "n_head = 4 # number of parallel self-attentions scores are calculated for each sentence.\n",
        "n_layer = 4 # number of layer of transformer a sentence pass through for deep understanding, like Input → Layer 1 → Layer 2 → Layer 3 → Layer 4 → Output\n",
        "dropout = 0.0 # how much neurons we forget during training, for preventing overfitting.\n",
        "\n",
        "#----------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# create bi-directional mapping from char to int\n",
        "stoi = { ch: i for i, ch in enumerate(chars) }\n",
        "itos = { i: ch for i, ch in enumerate(chars) }\n",
        "\n",
        "# ecoder and decode for string\n",
        "encode = lambda s: [stoi[ch] for ch in s] # take input of string and output list of integers.\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # take input of list of integers and output string.\n",
        "\n",
        "# Train and Test Data\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "  # generate the batch of input x and target y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "\n",
        "# evaluation of the model after certain steps of training\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "        X, Y = get_batch(split)\n",
        "        logits, loss = model(X, Y)\n",
        "        losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\"One head of self-attention\"\"\"\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.register_buffer('trill', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    # tokens attentions scores\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "    wei = wei.masked_fill(self.trill[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "    # tokens weight aggregation\n",
        "    v = self.value(x) # (B, T, C)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" Multiple-Head of self attention in parallel to capture more relations between tokens \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embed, n_embed)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "  \"\"\" A simple feed forward neural network for capturing more details\"\"\"\n",
        "\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4 * n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embed, n_embed),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer Block \"\"\"\n",
        "\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size) # calculate attention scores for each sequence and concatenate the n_head different scores in last dimension.\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape # number of batches B and each batch contains a sequence of T tokens.\n",
        "\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    token_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = token_emb + pos_emb # (B,T,C)\n",
        "    x = self.blocks(x) # (B,T,C)\n",
        "    x = self.ln_f(x) # (B,T,C)\n",
        "    logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      # this is the context window size, last context to remember\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx_cond)\n",
        "      # focus only on the last token in every sequence across batches\n",
        "      logits = logits[:, -1, :] # becomes (B, C) logits = [[vocab_scores_for_last_token_seq1], [vocab_scores_for_last_token_seq2], ...]\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) [[index1_token_1_seq1], [index2_token_2_seq2], [index3_token_3_seq3],...]\n",
        "      # append the next_idx to the previous idx\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer -> make previous gradients 0 -> compute gradients of loss w.r.t to model.parameters() [weights] -> backpropagate (update the weights)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# training\n",
        "for iter in range(max_iters):\n",
        "  # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train') # basically xb (B, T) -> input and yb -> output (B, T) [each token shifted by 1 offset corresponding to token in xb]\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hzRY58LH3r7a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}