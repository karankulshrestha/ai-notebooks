{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM477enqGoFF25TO1sphZo1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karankulshrestha/ai-notebooks/blob/main/concepts_attention_is_all_you_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yLljnQmUtr7L"
      },
      "outputs": [],
      "source": [
        "SENTENCE = \"AI IS FUTURE\"\n",
        "D_MODEL = 512"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "input_ids = torch.tensor([10, 20, 30])"
      ],
      "metadata": {
        "id": "SUnh2hvyvdFP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: The Embedding Matrix (The First Weights)\n",
        "\n",
        "The transformer model starts by converting each token ID into a dense vector using a large **lookup table** called the **Embedding Matrix**. This matrix contains trainable weights that the model learns during training.\n",
        "\n",
        "- **Rows**: Equal to the vocabulary size (e.g., 10,000 different tokens/words).\n",
        "- **Columns**: Equal to the model dimension $d_{\\text{model}}$ (e.g., 512 in the original Transformer – here we use a small value like 4 for illustration).\n",
        "\n",
        "#### Shape of the Embedding Matrix\n",
        "\n",
        "\n",
        "#### How it works\n",
        "For each input token ID (an integer), the model simply **looks up** (\"plucks\") the corresponding row from this matrix. That row becomes the initial vector representation of the token.\n",
        "\n",
        "**Example** (with tiny numbers for clarity):\n",
        "\n",
        "| Token ID | Token     | Embedding Vector (d_model = 4)      |\n",
        "|----------|-----------|-------------------------------------|\n",
        "| 0        | \\<pad\\>   | [0.12, -0.45, 0.67, 0.23]          |\n",
        "| 1        | hello     | [-0.34, 0.89, -0.12, 0.56]         |\n",
        "| 2        | world     | [0.78, -0.23, 0.45, -0.89]         |\n",
        "| ...      | ...       | ...                                 |\n",
        "| 9999     | !         | [0.01, 0.34, -0.67, 0.12]          |\n",
        "\n",
        "So, if the input token is \"hello\" (ID = 1), the model grabs the vector `[-0.34, 0.89, -0.12, 0.56]` directly from the embedding matrix.\n",
        "\n",
        "This simple lookup is the very first operation in the model and turns discrete token IDs into continuous vectors that the rest of the transformer can work with. During training, these embedding weights are updated so the model learns meaningful representations for each token."
      ],
      "metadata": {
        "id": "dBdelbkLxX9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights: [Vocab Size=100, d_model=4]\n",
        "W_embedding = torch.randn(100, 4)\n",
        "\n",
        "# Operation: Select rows 10, 20, and 30\n",
        "# Shape becomes: [3, 4] (3 words, each is a vector of size 4)\n",
        "x = W_embedding[input_ids]\n",
        "\n",
        "# Scale weights by sqrt(d_model) as per paper [cite: 159]\n",
        "x = x * math.sqrt(4)"
      ],
      "metadata": {
        "id": "kw4IrStMxXXW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Positional Encoding (Adding Order)\n",
        "\n",
        "Since we are performing matrix operations, the model has no inherent understanding that \"AI\" appears before \"is\" in the sequence. We must mathematically encode the order of tokens.\n",
        "\n",
        "# Positional Encoding Formula\n",
        "\n",
        "For a model with dimension $d_{model} = 4$, we use two indices to cover the columns: $2i$ (even columns) and $2i+1$ (odd columns).\n",
        "\n",
        "$$\\begin{aligned}\n",
        "PE_{(pos, 2i)} &= \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) \\\\\n",
        "PE_{(pos, 2i+1)} &= \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
        "\\end{aligned}$$\n",
        "\n",
        "Where:\n",
        "* $pos$ is the position of the word (0, 1, 2).\n",
        "* $i$ is the index of the frequency pair (0, 1).\n",
        "* $d_{model}$ is 4.\n",
        "\n",
        "### The Solution\n",
        "\n",
        "We create a fixed matrix of the same shape `[3, 4]` containing sine and cosine waves, then add it element-wise to our input embeddings.\n",
        "\n",
        "### Why This Works\n",
        "\n",
        "The sine and cosine functions with different frequencies create unique positional signatures for each position in the sequence, allowing the model to distinguish between tokens based on their location while maintaining mathematical properties that help with learning."
      ],
      "metadata": {
        "id": "xAISwnfAzML6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding Calculation for Position 1\n",
        "\n",
        "Since your example matrix is $3 \\times 4$ (3 positions, 4 dimensions), let's perform the actual Sinusoidal Positional Encoding calculation for Position 1 (the middle row). This will show you exactly how $i$ (the frequency pair index) drives the values for that specific row.\n",
        "\n",
        "## The Inputs\n",
        "\n",
        "- Current Position ($pos$): 1\n",
        "- Model Dimension ($d_{model}$): 4\n",
        "- Frequency Pairs ($i$): Since we have 4 columns, we have 2 pairs.\n",
        "  - $i = 0$ (controls columns 0 & 1)\n",
        "  - $i = 1$ (controls columns 2 & 3)\n",
        "\n",
        "## Step 1: Calculate for Pair $i = 0$ (Columns 0 & 1)\n",
        "\n",
        "**1. Calculate the Divisor (Frequency)**\n",
        "\n",
        "The formula for the divisor is $10000^{\\frac{2i}{d_{model}}}$.\n",
        "\n",
        "$$\\text{Divisor} = 10000^{\\frac{2(0)}{4}} = 10000^{0} = \\mathbf{1}$$\n",
        "\n",
        "**2. Calculate the Values**\n",
        "\n",
        "- Column 0 (Sine): $\\sin(\\frac{pos}{\\text{divisor}}) = \\sin(\\frac{1}{1}) = \\sin(1) \\approx \\mathbf{0.84}$\n",
        "- Column 1 (Cosine): $\\cos(\\frac{pos}{\\text{divisor}}) = \\cos(\\frac{1}{1}) = \\cos(1) \\approx \\mathbf{0.54}$\n",
        "\n",
        "**Result for $i=0$:** [0.84, 0.54]\n",
        "\n",
        "## Step 2: Calculate for Pair $i = 1$ (Columns 2 & 3)\n",
        "\n",
        "**1. Calculate the Divisor (Frequency)**\n",
        "\n",
        "$$\\text{Divisor} = 10000^{\\frac{2(1)}{4}} = 10000^{\\frac{2}{4}} = 10000^{0.5} = \\sqrt{10000} = \\mathbf{100}$$\n",
        "\n",
        "**2. Calculate the Values**\n",
        "\n",
        "- Column 2 (Sine): $\\sin(\\frac{pos}{\\text{divisor}}) = \\sin(\\frac{1}{100}) = \\sin(0.01) \\approx \\mathbf{0.01}$\n",
        "- Column 3 (Cosine): $\\cos(\\frac{pos}{\\text{divisor}}) = \\cos(\\frac{1}{100}) = \\cos(0.01) \\approx \\mathbf{1.00}$\n",
        "\n",
        "**Result for $i=1$:** [0.01, 1.00]\n",
        "\n",
        "## Step 3: The Final Row\n",
        "\n",
        "Combining the results from $i=0$ and $i=1$, the actual positional encoding vector for Position 1 is:\n",
        "\n",
        "$$[0.84, 0.54, 0.01, 1.00]$$\n",
        "\n",
        "## Comparison to Your Dummy Values\n",
        "\n",
        "You provided the dummy row: [0.3, 0.4, 0.3, 0.4].\n",
        "\n",
        "Notice the difference:\n",
        "\n",
        "- **Your Dummy:** The values repeated (0.3, 0.4 then 0.3, 0.4). This implies the frequency (divisor) was the same for both pairs.\n",
        "- **Actual Math:** The values changed drastically (0.84, 0.54 vs 0.01, 1.00). This is because $i$ increased, making the divisor larger ($1 \\rightarrow 100$), which slows down the frequency of the wave."
      ],
      "metadata": {
        "id": "nJgZkd8wi7Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed matrix of positions (simplified for demo)\n",
        "# Shape: [3, 4]\n",
        "position_matrix = torch.tensor([\n",
        "    [0.1, 0.2, 0.1, 0.2],  # Position 0\n",
        "    [0.3, 0.4, 0.3, 0.4],  # Position 1\n",
        "    [0.5, 0.6, 0.5, 0.6]   # Position 2\n",
        "])\n",
        "\n",
        "# Operation: Element-wise addition\n",
        "# Shape remains: [3, 4]\n",
        "x = x + position_matrix"
      ],
      "metadata": {
        "id": "hx32vUtdyMLW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Creating Query, Key, Value (The \"Projections\")\n",
        "\n",
        "Now we enter the **Self-Attention mechanism**—this is the core \"brain.\"\n",
        "\n",
        "### Three Distinct Views\n",
        "\n",
        "We need three different perspectives of our data:\n",
        "\n",
        "1. **Query** ($Q$): What I am looking for\n",
        "2. **Key** ($K$): What I contain (for others to search)\n",
        "3. **Value** ($V$): My actual content\n",
        "\n",
        "We create these by multiplying our input $x$ by three different weight matrices ($W_Q$, $W_K$, $W_V$).\n",
        "\n",
        "Each projection transforms the same input into a specialized representation that serves a specific purpose in the attention mechanism."
      ],
      "metadata": {
        "id": "cA8DS1cy0VWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly initialized weight matrices\n",
        "# Shape: [d_model=4, d_model=4]\n",
        "W_q = torch.randn(4, 4)\n",
        "W_k = torch.randn(4, 4)\n",
        "W_v = torch.randn(4, 4)\n",
        "\n",
        "# Operation: Matrix Multiplication (Dot Product)\n",
        "# Equation: Q = x @ W_q\n",
        "Q = x @ W_q  # Shape: [3, 4]\n",
        "K = x @ W_k  # Shape: [3, 4]\n",
        "V = x @ W_v  # Shape: [3, 4]"
      ],
      "metadata": {
        "id": "nIg6tbPc0BGl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: The Attention Scores (Relationships)\n",
        "\n",
        "We want to find out how much the word \"Future\" (Query) relates to \"AI\" (Key). We do this by calculating the **Dot Product** between every Query and every Key.\n",
        "\n",
        "### The Operation\n",
        "\n",
        "$$Q \\times K^T$$\n",
        "\n",
        "We transpose $K$ to align the dimensions properly for matrix multiplication.\n",
        "\n",
        "```python\n",
        "# Q shape: [3, 4]\n",
        "# K shape: [3, 4]\n",
        "# K^T shape: [4, 3]\n",
        "\n",
        "# Matrix multiplication: [3, 4] @ [4, 3] = [3, 3]\n",
        "attention_scores = Q @ K.T\n",
        "\n",
        "# Result: A [3, 3] matrix where each cell (i, j) represents\n",
        "# how much token i (query) relates to token j (key)\n",
        "```\n",
        "\n",
        "### Understanding the Result\n",
        "\n",
        "The resulting `[3, 3]` matrix contains similarity scores:\n",
        "- Each row represents one query (one word asking \"who should I pay attention to?\")\n",
        "- Each column represents one key (one word being searched)\n",
        "- Higher scores indicate stronger relationships between tokens"
      ],
      "metadata": {
        "id": "K6wYq3Ez6JKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose K: Swaps rows and columns\n",
        "K_transpose = K.t() # Shape: [4, 3]\n",
        "\n",
        "# Operation: Matrix Multiply\n",
        "# Shape: [3, 4] @ [4, 3] -> [3, 3]\n",
        "scores = Q @ K_transpose\n",
        "\n",
        "# The result is a 3x3 grid showing the relationship of every word to every other word.\n",
        "# Row 1 is \"AI\", Row 2 is \"is\", Row 3 is \"future\"."
      ],
      "metadata": {
        "id": "iHBAotuN5va2"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Scaling and Masking (The \"Decoder\" Logic)\n",
        "\n",
        "### Scaling: Preventing Gradient Issues\n",
        "\n",
        "We divide by the square root of the dimension size ($\\sqrt{d_k}$) to stop the numbers from getting too large and killing the gradients during backpropagation.\n",
        "\n",
        "```python\n",
        "# d_k = 4 in our example\n",
        "d_k = 4\n",
        "\n",
        "# Scale the attention scores\n",
        "attention_scores = attention_scores / math.sqrt(d_k)\n",
        "```\n",
        "\n",
        "**Why this matters:** Without scaling, large dot products push the softmax function into regions with extremely small gradients, making learning difficult.\n",
        "\n",
        "---\n",
        "\n",
        "### Masking: Enforcing Causality\n",
        "\n",
        "This is **crucial for GPT/Decoder models**. We cannot let the first word see the third word (because the third word is \"the future\"). We force those scores to be $-\\infty$.\n",
        "\n",
        "```python\n",
        "# Create a causal mask (lower triangular matrix)\n",
        "# Shape: [3, 3]\n",
        "mask = torch.triu(torch.ones(3, 3), diagonal=1).bool()\n",
        "\n",
        "# Apply mask: Set future positions to -inf\n",
        "attention_scores = attention_scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "# Result looks like:\n",
        "# [[score, -inf, -inf],   # Token 0 can only see itself\n",
        "#  [score, score, -inf],  # Token 1 can see 0 and 1\n",
        "#  [score, score, score]] # Token 2 can see all previous tokens\n",
        "```\n",
        "\n",
        "**The Logic:** When we apply softmax in the next step, $e^{-\\infty} = 0$, effectively zeroing out attention to future tokens. This ensures the model can only attend to previous and current positions—maintaining the autoregressive property that makes GPT a valid language model."
      ],
      "metadata": {
        "id": "a2eLMM6q7_Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = 4\n",
        "scores = scores / math.sqrt(d_k)\n",
        "mask = torch.triu(torch.ones(3, 3), diagonal=1).bool() # this makes all the elements in the specified diaognal (0, 1, 2) and below it as 0 (false)\n",
        "scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "# Result Ex:\n",
        "# [ 0.5, -inf, -inf ] -> Word 1 only sees Word 1\n",
        "# [ 0.2,  0.8, -inf ] -> Word 2 sees Word 1 and 2\n",
        "# [ 0.1,  0.4,  0.9 ] -> Word 3 sees all\n",
        "\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOnbpxdX6zAP",
        "outputId": "97d74103-cc8c-4356-eb1a-af3df0e29b9d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-7.5409,    -inf,    -inf],\n",
              "        [ 3.9556,  5.2303,    -inf],\n",
              "        [-7.4488,  4.2887, 10.1782]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Softmax (Probabilities)\n",
        "\n",
        "We convert these raw scores into **probabilities** that sum to 1. This tells the model \"pay 80% attention to word X and 20% to word Y\".\n",
        "\n",
        "### The Transformation\n",
        "\n",
        "```python\n",
        "# Apply softmax across the last dimension (each row independently)\n",
        "# Shape: [3, 3] -> [3, 3]\n",
        "attention_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "# Each row now sums to 1.0\n",
        "# Example result:\n",
        "# [[1.0,  0.0,  0.0 ],   # Token 0: 100% attention to itself\n",
        "#  [0.3,  0.7,  0.0 ],   # Token 1: 30% to token 0, 70% to itself\n",
        "#  [0.1,  0.2,  0.7 ]]   # Token 2: distributed across all three tokens\n",
        "```\n",
        "\n",
        "### What This Means\n",
        "\n",
        "Each row represents an **attention distribution** for one token:\n",
        "- Values are between 0 and 1\n",
        "- Each row sums to exactly 1.0\n",
        "- Higher values mean \"pay more attention to this token\"\n",
        "- The masking from Step 6 ensures future tokens have 0% attention (due to $e^{-\\infty} = 0$)\n",
        "\n",
        "This probability distribution determines how much information each token will gather from other tokens in the next step."
      ],
      "metadata": {
        "id": "w06mKU4P_S1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Operation: Softmax\n",
        "attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
        "\n",
        "# Now every row sums to 1.0"
      ],
      "metadata": {
        "id": "MkMK5J_z89FG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Weighted Sum (The Output of Attention)\n",
        "\n",
        "Now we create the **new representation** of each word. We multiply our probabilities (Attention Weights) by the actual content ($V$).\n",
        "\n",
        "### The Operation\n",
        "\n",
        "$$\\text{Output} = \\text{Weights} \\times V$$\n",
        "\n",
        "```python\n",
        "# attention_weights shape: [3, 3]\n",
        "# V shape: [3, 4]\n",
        "\n",
        "# Matrix multiplication: [3, 3] @ [3, 4] = [3, 4]\n",
        "attention_output = attention_weights @ V\n",
        "\n",
        "# Result: [3, 4] - same shape as our original input\n",
        "# Each token now contains information from all tokens it attended to\n",
        "```\n",
        "\n",
        "### What Just Happened\n",
        "\n",
        "Each token's new representation is a **weighted combination** of all the Value vectors it was allowed to attend to:\n",
        "\n",
        "- **Token 0**: Gets 100% of its own value (due to masking)\n",
        "- **Token 1**: Gets 30% of token 0's value + 70% of its own value\n",
        "- **Token 2**: Gets 10% of token 0 + 20% of token 1 + 70% of its own value\n",
        "\n",
        "The attention weights act as a **mixing recipe**, determining how much of each token's content to blend together. This is where the model learns relationships and context between words."
      ],
      "metadata": {
        "id": "jhEaLqS4CrRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape: [3, 3] @ [3, 4] -> [3, 4]\n",
        "attention_output = attention_weights @ V"
      ],
      "metadata": {
        "id": "6Tjrq4mECc4c"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Feed Forward Network (The \"Processing\")\n",
        "\n",
        "The attention mechanism just gathered information. Now the **Feed Forward Network (FFN)** processes it.\n",
        "\n",
        "The FFN is simply two linear transformations with a ReLU activation (removing negatives) in the middle.\n",
        "\n",
        "### The Mathematical Formula\n",
        "\n",
        "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
        "\n",
        "### The Implementation\n",
        "\n",
        "```python\n",
        "# Typical FFN expands then contracts\n",
        "# d_model = 4, d_ff = 16 (usually 4x larger)\n",
        "\n",
        "# First layer weights and bias\n",
        "W_1 = torch.randn(4, 16)  # Expand: [4] -> [16]\n",
        "b_1 = torch.randn(16)\n",
        "\n",
        "# Second layer weights and bias\n",
        "W_2 = torch.randn(16, 4)  # Contract: [16] -> [4]\n",
        "b_2 = torch.randn(4)\n",
        "\n",
        "# Forward pass\n",
        "# Step 1: Linear transformation + expand\n",
        "hidden = attention_output @ W_1 + b_1  # [3, 4] @ [4, 16] = [3, 16]\n",
        "\n",
        "# Step 2: ReLU activation (zero out negatives)\n",
        "hidden = torch.relu(hidden)  # [3, 16] -> [3, 16]\n",
        "\n",
        "# Step 3: Linear transformation + contract back\n",
        "ffn_output = hidden @ W_2 + b_2  # [3, 16] @ [16, 4] = [3, 4]\n",
        "```\n",
        "\n",
        "### What This Does\n",
        "\n",
        "The FFN provides **position-wise processing**:\n",
        "- **Expand**: Projects to a higher dimension ($d_{ff} = 4 \\times d_{model}$) to increase representational capacity\n",
        "- **ReLU**: Introduces non-linearity, allowing the network to learn complex patterns\n",
        "- **Contract**: Projects back to the original dimension ($d_{model}$)\n",
        "\n",
        "Each token is processed **independently** through the same FFN, allowing the model to refine the representations after attention has mixed information between tokens."
      ],
      "metadata": {
        "id": "lJ-Zd4scDt1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = torch.randn(4, 16)\n",
        "b1 = torch.zeros(16)\n",
        "W2 = torch.randn(16, 4)\n",
        "b2 = torch.zeros(4)\n",
        "\n",
        "\n",
        "# 1. First Expansion\n",
        "# Shape: [3, 4] @ [4, 16] -> [3, 16]\n",
        "hidden = attention_output @ W1 + b1\n",
        "\n",
        "# 2. ReLU Activation (Non-linearity)\n",
        "hidden = torch.relu(hidden)\n",
        "\n",
        "# 3. Projection Back\n",
        "# Shape: [3, 16] @ [16, 4] -> [3, 4]\n",
        "layer_output = hidden @ W2 + b2"
      ],
      "metadata": {
        "id": "b7A0PLaEDsFz"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Final Prediction (Logits)\n",
        "\n",
        "- After passing through $N$ layers of the above logic (repeating Steps 4-9), we project the final vector back to the vocabulary size to predict the next word."
      ],
      "metadata": {
        "id": "KGm8wgW_FdG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unembedding Matrix (Output Head)\n",
        "# Shape: [4, 100] (Projects back to vocab size)\n",
        "W_unembed = torch.randn(4, 100)\n",
        "\n",
        "# Operation: Matrix Multiply\n",
        "# Shape: [3, 4] @ [4, 100] -> [3, 100]\n",
        "logits = layer_output @ W_unembed\n",
        "\n",
        "temperature = 0.7\n",
        "scaled_logits = logits / temperature\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "probs = F.softmax(scaled_logits, dim=-1)\n",
        "\n",
        "print(\"Probabilities:\", probs)\n",
        "\n",
        "predicted_token_ids = torch.argmax(probs, dim=-1)\n",
        "\n",
        "print(\"Predicted Token IDs:\", predicted_token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZP9vxPrEVkm",
        "outputId": "2ce8cf64-3612-498a-b692-9442163e628a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities: tensor([[0.0000e+00, 2.7273e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 1.3735e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7868e-32, 9.3156e-38,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4876e-25, 9.9800e-17,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 5.2232e-33, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         1.3460e-14, 5.2254e-34, 0.0000e+00, 1.6853e-31, 0.0000e+00, 0.0000e+00,\n",
            "         1.6955e-31, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5662e-18, 9.1214e-01, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 6.9784e-34, 0.0000e+00, 0.0000e+00, 4.3503e-13,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6073e-42,\n",
            "         0.0000e+00, 8.7581e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 2.2121e-21, 7.0482e-10, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 3.7320e-29, 1.2707e-39, 0.0000e+00],\n",
            "        [2.7033e-13, 1.0498e-14, 1.3048e-06, 3.1069e-08, 6.1492e-07, 6.2629e-11,\n",
            "         3.1453e-06, 3.1214e-15, 2.6437e-10, 2.7920e-10, 2.9042e-09, 2.8097e-04,\n",
            "         1.8468e-03, 3.4838e-04, 1.6800e-02, 8.6791e-08, 2.5317e-15, 2.6199e-08,\n",
            "         1.2723e-02, 6.5625e-07, 1.9781e-05, 4.6435e-14, 9.6213e-09, 6.4574e-02,\n",
            "         5.4854e-16, 6.4017e-13, 5.2561e-08, 3.7711e-07, 2.1722e-15, 1.1440e-10,\n",
            "         1.2272e-05, 3.5132e-09, 1.5793e-08, 2.0139e-06, 3.8136e-09, 8.8115e-01,\n",
            "         4.7706e-08, 3.6440e-20, 4.4255e-09, 7.1541e-09, 1.8689e-02, 3.6814e-09,\n",
            "         2.3301e-12, 7.0342e-12, 1.1906e-07, 1.6438e-11, 4.0076e-12, 3.8640e-08,\n",
            "         4.5500e-08, 5.1860e-09, 1.7300e-06, 2.1019e-04, 7.7248e-16, 8.9920e-07,\n",
            "         9.0572e-13, 8.9301e-07, 2.4376e-10, 3.2262e-14, 1.6629e-08, 4.5703e-12,\n",
            "         3.9804e-08, 2.8820e-03, 1.0190e-07, 3.3068e-14, 2.8402e-12, 2.0407e-12,\n",
            "         6.5703e-07, 8.6519e-08, 2.0087e-14, 5.7686e-11, 1.9278e-14, 1.4144e-12,\n",
            "         1.6272e-07, 1.6976e-06, 2.6507e-09, 3.9043e-12, 2.1465e-08, 4.6790e-05,\n",
            "         1.9906e-09, 3.4159e-12, 1.9778e-11, 3.3530e-08, 4.1158e-05, 7.9254e-06,\n",
            "         1.5533e-12, 3.3134e-07, 2.8478e-04, 1.2667e-10, 1.8837e-09, 3.8551e-06,\n",
            "         8.1898e-09, 1.2295e-08, 8.6045e-14, 2.1269e-12, 2.2909e-10, 6.6002e-05,\n",
            "         7.0354e-12, 1.0361e-08, 4.7785e-09, 2.6220e-11],\n",
            "        [7.4753e-39, 0.0000e+00, 2.7785e-22, 5.8491e-31, 3.4748e-26, 9.1766e-34,\n",
            "         1.7775e-12, 0.0000e+00, 2.3303e-27, 1.0161e-40, 2.8230e-31, 1.6074e-06,\n",
            "         9.0605e-06, 4.0131e-20, 6.8110e-12, 2.1534e-23, 0.0000e+00, 1.1627e-24,\n",
            "         3.9612e-11, 1.1407e-06, 5.1803e-23, 0.0000e+00, 1.2113e-32, 6.2530e-04,\n",
            "         0.0000e+00, 1.4153e-43, 1.5011e-30, 2.8139e-28, 0.0000e+00, 5.6817e-33,\n",
            "         1.2109e-16, 7.5509e-34, 3.5483e-26, 7.1365e-16, 1.8720e-35, 9.9936e-01,\n",
            "         9.2675e-32, 0.0000e+00, 7.8782e-25, 2.3947e-25, 2.1415e-08, 1.0323e-30,\n",
            "         1.1210e-44, 1.9486e-39, 6.2906e-16, 9.0689e-26, 1.5305e-40, 1.8196e-32,\n",
            "         1.8499e-22, 7.1287e-22, 2.5344e-13, 5.7049e-22, 0.0000e+00, 1.0325e-24,\n",
            "         0.0000e+00, 1.6661e-29, 5.7778e-37, 0.0000e+00, 7.6428e-22, 4.1139e-41,\n",
            "         2.3033e-29, 2.5916e-13, 3.9091e-22, 0.0000e+00, 1.3873e-43, 2.2421e-43,\n",
            "         7.7730e-30, 3.1836e-27, 0.0000e+00, 4.0449e-38, 0.0000e+00, 7.1746e-43,\n",
            "         9.6776e-17, 2.9028e-15, 2.2427e-22, 5.6052e-45, 4.0809e-18, 1.3461e-11,\n",
            "         4.4450e-27, 1.4013e-45, 1.0735e-38, 1.7933e-24, 2.2107e-12, 4.4919e-22,\n",
            "         8.9683e-44, 2.3599e-21, 8.5174e-09, 7.8183e-32, 3.8730e-27, 2.3086e-21,\n",
            "         1.5534e-36, 5.4027e-28, 8.4919e-42, 9.6344e-37, 4.6833e-32, 3.5352e-21,\n",
            "         0.0000e+00, 3.3996e-19, 1.2756e-29, 1.1672e-38]])\n",
            "Predicted Token IDs: tensor([58, 35, 35])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADD & NORM BASIC_PRINCIPLES"
      ],
      "metadata": {
        "id": "uOWYGEXDJxQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Output\n",
        "# [[ 0.5,  0.5, -1.0 ],   <- Update for Word 1\n",
        "# [ 0.2, -0.2,  0.0 ]]   <- Update for Word 2\n",
        "\n",
        "# Define the tensors\n",
        "x = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "attn_out = torch.tensor([[0.5, 0.5, -1.0], [0.2, -0.2, 0.0]])\n",
        "\n",
        "# THE RESIDUAL ADDITION\n",
        "added_x = x + attn_out\n",
        "\n",
        "print(\"After Add:\")\n",
        "print(added_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRgdEhraFyQV",
        "outputId": "9032f2e7-556f-46b5-fd01-98119280c594"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Add:\n",
            "tensor([[1.5000, 2.5000, 2.0000],\n",
            "        [4.2000, 4.8000, 6.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: The \"Norm\" (Layer Normalization)\n",
        "\n",
        "We normalize each word (row) independently. We want every word vector to have a **Mean** of $\\approx 0$ and a **Standard Deviation** of $\\approx 1$.\n",
        "\n",
        "---\n",
        "\n",
        "### 2a. Calculate Mean & Variance (Per Row)\n",
        "\n",
        "#### Analyzing Row 1: `[1.5, 2.5, 2.0]`\n",
        "\n",
        "**1. Mean:**\n",
        "$$\\frac{1.5 + 2.5 + 2.0}{3} = 2.0$$\n",
        "\n",
        "**2. Variance:** How far is each number from the mean (2.0)?\n",
        "- $(1.5 - 2.0)^2 = (-0.5)^2 = 0.25$\n",
        "- $(2.5 - 2.0)^2 = (0.5)^2 = 0.25$\n",
        "- $(2.0 - 2.0)^2 = (0.0)^2 = 0.00$\n",
        "- Average Variance = $0.50 / 3 \\approx 0.166$\n",
        "\n",
        "**3. Standard Deviation:**\n",
        "$$\\sqrt{0.166} \\approx 0.408$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Analyzing Row 2: `[4.2, 4.8, 6.0]`\n",
        "\n",
        "**1. Mean:** $5.0$\n",
        "\n",
        "**2. Standard Deviation:** (Calculated similarly) $\\approx 0.748$\n",
        "\n",
        "---\n",
        "\n",
        "### 2b. Normalize (Shift & Scale)\n",
        "\n",
        "**Formula:**\n",
        "$$\\frac{x - \\text{mean}}{\\text{std\\_dev} + \\epsilon}$$\n",
        "\n",
        "We add a tiny $\\epsilon$ (like $10^{-5}$) to avoid dividing by zero.\n",
        "\n",
        "---\n",
        "\n",
        "#### Normalizing Row 1:\n",
        "\n",
        "- Value 1: $\\frac{1.5 - 2.0}{0.408} = -1.22$\n",
        "- Value 2: $\\frac{2.5 - 2.0}{0.408} = +1.22$\n",
        "- Value 3: $\\frac{2.0 - 2.0}{0.408} = 0.00$\n",
        "\n",
        "---\n",
        "\n",
        "#### Normalizing Row 2:\n",
        "\n",
        "- Value 1: $\\frac{4.2 - 5.0}{0.748} = -1.07$\n",
        "- Value 2: $\\frac{4.8 - 5.0}{0.748} = -0.27$\n",
        "- Value 3: $\\frac{6.0 - 5.0}{0.748} = +1.33$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Y_AGknMIKS3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Calculate Mean (keepdim=True ensures we get [[mean1], [mean2]])\n",
        "mean = added_x.mean(dim=-1, keepdim=True)\n",
        "# Result: [[2.0], [5.0]]\n",
        "\n",
        "# 2. Calculate Std Dev\n",
        "std = added_x.std(dim=-1, keepdim=True, unbiased=False)\n",
        "# Result: [[0.4082], [0.7483]]\n",
        "\n",
        "# 3. Normalize\n",
        "normalized_x = (added_x - mean) / (std + 1e-5)\n",
        "\n",
        "print(\"After Normalization:\")\n",
        "print(normalized_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2j1gnhLKLRe",
        "outputId": "1583cf0b-daa1-4054-907d-f71f381fd6f9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Normalization:\n",
            "tensor([[-1.2247,  1.2247,  0.0000],\n",
            "        [-1.0690, -0.2673,  1.3363]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Concept"
      ],
      "metadata": {
        "id": "ilzklasaLUbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Head Attention changes the shape of the data so we run several smaller attention mechanisms in parallel.\n",
        "\n",
        "## The Goal\n",
        "\n",
        "We want to split our d_model (4) into 2 Heads of size 2.\n",
        "\n",
        "- Head 1 focuses on the first 2 numbers (maybe \"grammar\").\n",
        "- Head 2 focuses on the last 2 numbers (maybe \"meaning\")."
      ],
      "metadata": {
        "id": "EdEg0U4unO3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 3, 4)\n",
        "\n",
        "\n",
        "d_model = 4\n",
        "n_head = 2\n",
        "head_dim = d_model // n_head # 4 / 2 = 2\n",
        "\n",
        "# Weights (Total size is still 4x4)\n",
        "W_q = torch.randn(d_model, d_model)\n",
        "W_k = torch.randn(d_model, d_model)\n",
        "W_v = torch.randn(d_model, d_model)\n",
        "W_o = torch.randn(d_model, d_model) # The output projection\n",
        "\n",
        "\n",
        "# 1. Project to Q, K, V (Standard Matrix Multiplication)\n",
        "# shape [1, 3, 4]\n",
        "Q = x @ W_q\n",
        "K = x @ W_k\n",
        "V = x @ W_v"
      ],
      "metadata": {
        "id": "0p_prIubK6wN"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCoYltOLniWx",
        "outputId": "26a57f0b-7d41-4fec-b7c8-f928e4f40f34"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0454, -0.9407,  0.4785,  0.4870],\n",
              "         [-1.8014, -0.4781, -0.9136, -1.0884],\n",
              "         [-1.2597,  1.5056, -1.7328, -2.7407]]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: The \"Split\" (Reshape & Transpose) [MAGIC BEGINS]\n",
        "\n",
        "# The Operation\n",
        "\n",
        "1. **View (Reshape):** Split the `4` into `2` (heads) $\\times$ `2` (head_dim).\n",
        "   - Old Shape: `[1, 3, 4]`\n",
        "   - New Shape: `[1, 3, 2, 2]`\n",
        "\n",
        "2. **Transpose (Swap):** We want the \"Head\" dimension to be before the \"Sequence\" dimension so PyTorch treats each head as a separate batch.\n",
        "   - Swap dim 1 (Seq) and dim 2 (Head).\n",
        "   - Final Shape: `[1, 2, 3, 2]` $\\rightarrow$ `[Batch, Heads, Seq, Head_Dim]`"
      ],
      "metadata": {
        "id": "X-A0dLfdokg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q = Q.view(1, 3, 2, 2)\n",
        "K = K.view(1, 3, 2, 2)\n",
        "V = V.view(1, 3, 2, 2)\n",
        "\n",
        "# transpose (Swap Heads and Seq)\n",
        "# [Batch, Heads, Seq, Head_Dim] -> [1, 2, 3, 2]\n",
        "Q = Q.transpose(1, 2)\n",
        "K = K.transpose(1, 2)\n",
        "V = V.transpose(1, 2)\n",
        "\n",
        "print(\"Shape after split:\", Q.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb8iEny4oerS",
        "outputId": "13dd2a8c-a2b2-4766-a852-7a0034b01794"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after split: torch.Size([1, 2, 3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Scaled Dot-Product Attention (Parallel)"
      ],
      "metadata": {
        "id": "Odpb_ipyq9XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. MatMul Q and K Transposed\n",
        "# We transpose the LAST two dimensions only (Seq and Head_Dim)\n",
        "# [1, 2, 3, 2] @ [1, 2, 2, 3] -> [1, 2, 3, 3]\n",
        "scores = Q @ K.transpose(-2, -1)\n",
        "\n",
        "# 2. Scale\n",
        "scores = scores / math.sqrt(head_dim)\n",
        "\n",
        "# 3. Mask (Optional but standard for Decoder)\n",
        "# We apply the same mask to both heads\n",
        "mask = torch.tril(torch.ones(3, 3))\n",
        "scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "# 4. Softmax\n",
        "attn_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "# 5. Multiply by V\n",
        "# [1, 2, 3, 3] @ [1, 2, 3, 2] -> [1, 2, 3, 2]\n",
        "attn_output = attn_weights @ V"
      ],
      "metadata": {
        "id": "u_hIXZwSpzDp"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: The \"Concat\" (Merging)\n",
        "\n",
        "We need to glue the heads back together to get our original shape `[1, 3, 4]` back.\n",
        "\n",
        "## The Operation\n",
        "\n",
        "1. **Transpose Back:** Swap `Head` and `Seq` again. $\\rightarrow$ `[1, 3, 2, 2]`\n",
        "2. **Contiguous:** Fix memory layout (required by PyTorch after transpose).\n",
        "3. **View (Flatten):** Smash the last two dimensions (`2` and `2`) back into `4`."
      ],
      "metadata": {
        "id": "9OCV9I-DsScv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "\n",
        "final_output = attn_output.view(1, 3, d_model)\n",
        "\n",
        "print(\"Shape after merge:\", final_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuWVBknmrXQa",
        "outputId": "69c59ebd-93a8-4f3c-cee8-934d10b5c854"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after merge: torch.Size([1, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_output = final_output @ W_o"
      ],
      "metadata": {
        "id": "seJW0QE2tkSU"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kn-z0kijtpjq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}