{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJlDY0FJArVPkitY15rqTn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karankulshrestha/ai-notebooks/blob/main/Neural_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Optimization?\n",
        "The process of making your model learn by finding the best parameters to minimize error"
      ],
      "metadata": {
        "id": "JYGUkkgADmzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Core Problem\n",
        "Neural networks are non-convex functions with complex landscapes. Neural networks are non-convex functions, meaning their loss surface (the graph of loss vs. parameters) is bumpy and full of hills and valleys — not a smooth bowl shape.\n",
        "\n",
        "There are many peaks and dips (local minima and maxima) instead of one single lowest point."
      ],
      "metadata": {
        "id": "eZ1_YqezFhpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Challenges:\n",
        "\n",
        "Local Minima vs Global Minima: Non-convex functions have many local minima\n",
        "\n",
        "*   Saddle Points: More common than local minima in high dimensions\n",
        "*   Poor Conditioning: Loss surface shaped like long, narrow valleys\n",
        "*   Vanishing/Exploding Gradients: Gradients become very tiny → the weights stop changing → the network learns very slowly or not at all. Gradients become very large → weights change too much → training becomes unstable or blows up.\n",
        "*   Cliffs: Extremely steep regions that can \"catapult\" parameters far away\n",
        "*   Modern Reality: For large networks, most local minima are actually \"good enough\" - we don't need the global minimum\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/format:webp/1*f9a162GhpMbiTVTAua_lLQ.png\" alt=\"Loss surface of a neural network\" width=\"500\" height=\"400\">\n",
        "\n",
        "\n",
        "x-axis → θ₀\n",
        "\n",
        "y-axis → θ₁\n",
        "These are the parameters (weights) of your model.\n",
        "\n",
        "z-axis → J(θ₀, θ₁)\n",
        "This is the loss (cost function) — how wrong your model’s predictions are for that combination of parameters."
      ],
      "metadata": {
        "id": "SRbB2YjrGNe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Descent (SGD)"
      ],
      "metadata": {
        "id": "Jit50ae7I-rF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://julienharbulot.com/images/gradient-descent/gradient-vector.png\" alt=\"Gradient vector illustration\" width=\"500\" height=\"400\">\n",
        "\n",
        "## How It Works\n",
        "- **Sample:** Pick a small random batch of data  \n",
        "- **Compute:** Calculate gradient (direction of steepest increase)  \n",
        "- **Update:** Move parameters in the opposite direction (downhill)\n",
        "\n",
        "### The Math\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\varepsilon \\times \\nabla J(\\theta)\n",
        "$$\n",
        "**Where:**  \n",
        "- $\\theta$ = parameters  \n",
        "- $\\varepsilon$ = learning rate (step size)  \n",
        "- $\\nabla J(\\theta)$ = gradient (slope)\n",
        "\n",
        "---\n",
        "\n",
        "### Learning Rate Rules\n",
        "- **Too High:** Oscillates wildly, may never converge  \n",
        "- **Too Low:** Learns very slowly, may get stuck  \n",
        "- **Must Decrease Over Time:**  \n",
        "  Most implementations use $\\varepsilon_k = \\varepsilon_0 \\times (1 - k / \\tau)$\n"
      ],
      "metadata": {
        "id": "xrVBHxtNJ8CU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Momentum - The Speed Booster\n",
        "\n",
        "**What It Does:** Helps SGD move faster through flat regions and narrow valleys\n",
        "\n",
        "### How It Works:\n",
        "- **Accumulate:** Keep track of past gradients (like velocity)\n",
        "- **Accelerate:** Continue moving in the same direction if gradients point the same way\n",
        "- **Dampen:** Slow down if gradients change direction\n",
        "\n",
        "### The Math:\n",
        "$$\n",
        "\\begin{align}\n",
        "v &\\leftarrow \\alpha v - \\varepsilon \\nabla J(\\theta) \\quad \\text{(velocity update)} \\\\\n",
        "\\theta &\\leftarrow \\theta + v \\quad \\text{(parameter update)}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "- $\\alpha$ = momentum coefficient (usually 0.9)\n",
        "- $v$ = velocity vector\n",
        "- $\\varepsilon$ = learning rate\n",
        "- $\\theta$ = parameters\n",
        "\n",
        "### Physical Intuition:\n",
        "Like a ball rolling down a hill — gains speed going downhill, slows when going uphill\n",
        "\n",
        "### When to Use:\n",
        "When SGD is too slow or gets stuck in narrow valleys"
      ],
      "metadata": {
        "id": "puRVgIThPPV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## AdaGrad - The Smart Scaler\n",
        "\n",
        "**What It Does:** Adapts learning rate for each parameter individually\n",
        "\n",
        "### How It Works:\n",
        "- **History Track:** Keep running sum of squared gradients for each parameter\n",
        "- **Smart Scaling:** Parameters with large gradients get smaller learning rates\n",
        "- **Gentle Progress:** Parameters with small gradients keep larger learning rates\n",
        "\n",
        "### The Math:\n",
        "$$\n",
        "\\begin{align}\n",
        "r &\\leftarrow r + g \\odot g \\quad \\text{(accumulate squared gradients)} \\\\\n",
        "\\Delta\\theta &\\leftarrow -\\frac{\\varepsilon}{\\sqrt{r + \\delta}} \\odot g \\quad \\text{(adaptive update)} \\\\\n",
        "\\theta &\\leftarrow \\theta + \\Delta\\theta \\quad \\text{(parameter update)}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "- $r$ = accumulated squared gradients (element-wise)\n",
        "- $g$ = current gradient $\\nabla J(\\theta)$\n",
        "- $\\varepsilon$ = global learning rate\n",
        "- $\\delta$ = small constant for numerical stability (typically $10^{-7}$)\n",
        "- $\\odot$ = element-wise multiplication\n",
        "\n",
        "### When to Use:\n",
        "Problems where different parameters need different learning rates"
      ],
      "metadata": {
        "id": "JqDDhZR0P0ld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## RMSprop - The Adaptive Memory\n",
        "\n",
        "**What It Does:** Like AdaGrad but uses exponential moving average instead of full history\n",
        "\n",
        "### Why Better Than AdaGrad:\n",
        "- **Problem:** AdaGrad accumulates all past gradients, making learning rate too small\n",
        "- **Solution:** RMSprop \"forgets\" old gradients using exponential decay\n",
        "\n",
        "### The Math:\n",
        "$$\n",
        "\\begin{align}\n",
        "r &\\leftarrow \\rho r + (1-\\rho) g \\odot g \\quad \\text{(exponential moving average)} \\\\\n",
        "\\Delta\\theta &\\leftarrow -\\frac{\\varepsilon}{\\sqrt{r + \\delta}} \\odot g \\quad \\text{(adaptive update)} \\\\\n",
        "\\theta &\\leftarrow \\theta + \\Delta\\theta \\quad \\text{(parameter update)}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "- $r$ = exponential moving average of squared gradients\n",
        "- $g$ = current gradient $\\nabla J(\\theta)$\n",
        "- $\\rho$ = decay rate (typically 0.9)\n",
        "- $\\varepsilon$ = global learning rate\n",
        "- $\\delta$ = small constant for numerical stability (typically $10^{-7}$)\n",
        "- $\\odot$ = element-wise multiplication\n",
        "\n",
        "### When to Use:\n",
        "Most popular adaptive method in deep learning"
      ],
      "metadata": {
        "id": "iFOZfZIyR8b2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Adam - The Adaptive Champion\n",
        "\n",
        "**What It Does:** Combines momentum and adaptive learning rates with bias correction\n",
        "\n",
        "### Key Features:\n",
        "- **First Moment:** Uses momentum-like term (exponential average of gradients)\n",
        "- **Second Moment:** Uses adaptive learning rate (exponential average of squared gradients)\n",
        "- **Bias Correction:** Fixes initialization bias in both moments\n",
        "\n",
        "### The Math:\n",
        "$$\n",
        "\\begin{align}\n",
        "s &\\leftarrow \\rho_1 s + (1-\\rho_1) g \\quad \\text{(first moment)} \\\\\n",
        "r &\\leftarrow \\rho_2 r + (1-\\rho_2) g \\odot g \\quad \\text{(second moment)} \\\\\n",
        "\\hat{s} &\\leftarrow \\frac{s}{1-\\rho_1^t} \\quad \\text{(bias correction)} \\\\\n",
        "\\hat{r} &\\leftarrow \\frac{r}{1-\\rho_2^t} \\quad \\text{(bias correction)} \\\\\n",
        "\\Delta\\theta &\\leftarrow -\\frac{\\varepsilon}{\\sqrt{\\hat{r}} + \\delta} \\odot \\hat{s} \\quad \\text{(update)} \\\\\n",
        "\\theta &\\leftarrow \\theta + \\Delta\\theta \\quad \\text{(parameter update)}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "- $s$ = first moment estimate (momentum)\n",
        "- $r$ = second moment estimate (adaptive learning rate)\n",
        "- $\\hat{s}$ = bias-corrected first moment\n",
        "- $\\hat{r}$ = bias-corrected second moment\n",
        "- $g$ = current gradient $\\nabla J(\\theta)$\n",
        "- $t$ = timestep\n",
        "- $\\odot$ = element-wise multiplication\n",
        "\n",
        "### Hyperparameters:\n",
        "- $\\rho_1 = 0.9$ (first moment decay)\n",
        "- $\\rho_2 = 0.999$ (second moment decay)\n",
        "- $\\varepsilon = 0.001$ (learning rate)\n",
        "- $\\delta = 10^{-8}$ (numerical stability constant)\n",
        "\n",
        "### When to Use:\n",
        "Generally robust default choice for most problems"
      ],
      "metadata": {
        "id": "mFobKICkSRx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Newton's Method - The Quadratic Approximation\n",
        "\n",
        "**What It Does:** Uses second derivatives (Hessian) for faster convergence\n",
        "\n",
        "### How It Works:\n",
        "- **Local Approximation:** Approximate function as quadratic curve near current point\n",
        "- **Direct Jump:** Jump directly to minimum of that quadratic approximation\n",
        "\n",
        "### The Math:\n",
        "$$\n",
        "\\theta^* = \\theta_0 - H^{-1} \\nabla J(\\theta_0) \\quad \\text{(Newton's update)}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "- $\\theta^*$ = new parameter values\n",
        "- $\\theta_0$ = current parameter values\n",
        "- $H$ = Hessian matrix (matrix of all second derivatives)\n",
        "- $H^{-1}$ = inverse of the Hessian\n",
        "- $\\nabla J(\\theta_0)$ = gradient at current point\n",
        "\n",
        "### Visual Comparison:\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/da/Newton_optimization_vs_grad_descent.svg\"\n",
        "     alt=\"Newton's Method vs Gradient Descent\"\n",
        "     width=\"200\"\n",
        "     height=\"auto\">\n",
        "\n",
        "### Problems:\n",
        "- **Expensive:** Requires computing inverse Hessian ($O(n^3)$ for $n$ parameters)\n",
        "- **Hessian Issues:** Needs positive definite Hessian, fails at saddle points\n",
        "\n",
        "---\n",
        "\n",
        "## Simple Explanation\n",
        "\n",
        "Think of optimization methods as ways to find the bottom of a valley:\n",
        "\n",
        "**Regular gradient descent** is like a blindfolded person taking small steps downhill. They feel which way is steepest and take a step in that direction. Simple, but slow.\n",
        "\n",
        "**Newton's Method** is like having a detailed map of the nearby terrain. Instead of just knowing \"which way is down,\" you know the exact shape of the valley around you. This lets you calculate exactly where the bottom should be and jump straight there in one step.\n",
        "\n",
        "### The Catch:\n",
        "\n",
        "1. **Expensive to compute:** Creating that detailed map (the Hessian) requires measuring the curvature in every direction. For a model with millions of parameters, this is like measuring millions × millions of curvature values, then solving a huge system of equations. It's computationally impractical.\n",
        "\n",
        "2. **Can be fooled:** The method assumes the valley is bowl-shaped (positive definite Hessian). But at saddle points (like a mountain pass), the terrain curves up in some directions and down in others. Newton's Method gets confused and might jump in the wrong direction.\n",
        "\n",
        "### When to Use:\n",
        "Only practical for small problems (few hundred parameters at most). In deep learning, we use approximations like L-BFGS or adaptive methods like Adam instead."
      ],
      "metadata": {
        "id": "kzykDgR8VkKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Quasi-Newton Methods - The Practical Second-Order\n",
        "\n",
        "**What They Do:** Approximate Newton's method without computing full Hessian\n",
        "\n",
        "### How They Work:\n",
        "- **Smart Approximation:** Build an approximation of the Hessian using only gradient information\n",
        "- **Iterative Updates:** Update the Hessian approximation with each step instead of computing it from scratch\n",
        "- **Balance:** Get most benefits of Newton's method at a fraction of the cost\n",
        "\n",
        "### The Math:\n",
        "\n",
        "**BFGS (Broyden-Fletcher-Goldfarb-Shanno):**\n",
        "$$\n",
        "\\begin{align}\n",
        "\\theta_{t+1} &= \\theta_t - \\alpha_t H_t^{-1} \\nabla J(\\theta_t) \\\\\n",
        "H_{t+1} &= H_t + \\text{correction based on gradient changes}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**L-BFGS (Limited-memory BFGS):**\n",
        "- Stores only recent gradient history (typically last 10-20 steps)\n",
        "- Memory: $O(nm)$ instead of $O(n^2)$ where $m$ is history size\n",
        "\n",
        "**Where:**\n",
        "- $H_t$ = approximate Hessian at step $t$\n",
        "- $\\alpha_t$ = learning rate (often from line search)\n",
        "- $\\nabla J(\\theta_t)$ = gradient at step $t$\n",
        "\n",
        "### Visual: L-BFGS Optimization Process\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Zhiwen-Hu-4/publication/349537968/figure/fig2/AS:11431281176031871@1689988798867/The-gradient-ascent-process-of-by-L-BFGS-optimization-algorithm.png\" alt=\"L-BFGS Optimization Process\" width=\"400\"\n",
        "     height=\"auto\">\n",
        "\n",
        "### Popular Variants:\n",
        "- **BFGS:** Full Hessian approximation, good for small to medium problems\n",
        "- **L-BFGS:** Memory-efficient version, practical for large problems\n",
        "- **SR1 (Symmetric Rank-1):** Simpler update rule, less stable\n",
        "\n",
        "### Advantages:\n",
        "- Faster convergence than first-order methods (gradient descent, Adam)\n",
        "- More practical than Newton's method (no need to compute or invert Hessian)\n",
        "- Works well on smooth optimization problems\n",
        "\n",
        "### Disadvantages:\n",
        "- Still more expensive per iteration than gradient descent\n",
        "- Requires storing history (though L-BFGS minimizes this)\n",
        "- Less effective on non-smooth or stochastic problems\n",
        "\n",
        "---\n",
        "\n",
        "## Simple Explanation\n",
        "\n",
        "Remember how **Newton's Method** needs an expensive detailed map (the Hessian) of the terrain? And **gradient descent** only knows which way is downhill at each step?\n",
        "\n",
        "**Quasi-Newton methods** are the clever middle ground:\n",
        "\n",
        "Imagine you're hiking down a mountain and keeping notes. Instead of measuring every curve in the terrain (Newton's expensive map), you:\n",
        "1. Take a step and note how the slope changed\n",
        "2. Use those notes to guess what the terrain looks like\n",
        "3. Your guess gets better with each step\n",
        "\n",
        "It's like building a map as you go, learning from experience rather than measuring everything upfront.\n",
        "\n",
        "### Real-World Analogy:\n",
        "\n",
        "Think of learning to ride a bike on different hills:\n",
        "- **Gradient descent:** You only feel if you're going up or down right now\n",
        "- **Newton's method:** You have a complete topographic map (expensive to make)\n",
        "- **Quasi-Newton:** You remember how the hills felt on previous rides and use that experience to predict what's coming\n",
        "\n",
        "### The L-BFGS Trick:\n",
        "\n",
        "Regular BFGS remembers everything, which gets expensive. L-BFGS is like only keeping your last 10-20 notes instead of a massive journal. You lose some detail, but it's much more practical and still works great.\n",
        "\n",
        "### When to Use:\n",
        "- **BFGS:** Medium-sized problems (thousands of parameters) with smooth objectives\n",
        "- **L-BFGS:** Larger problems (tens of thousands to millions of parameters) where you need better convergence than gradient descent but can't afford full Newton's method\n",
        "- **Deep Learning:** Rarely used for training neural networks (mini-batch stochasticity breaks the assumptions), but sometimes used for fine-tuning or on specific layers"
      ],
      "metadata": {
        "id": "ipji7z50YAlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## PART 5: INITIALIZATION STRATEGIES\n",
        "\n",
        "### Weight Initialization - The Foundation\n",
        "\n",
        "**Why It Matters:** Initial parameters determine whether training succeeds or fails\n",
        "\n",
        "#### Glorot (Xavier) Initialization:\n",
        "\n",
        "$$\n",
        "W_{ij} \\sim U\\left(-\\sqrt{\\frac{6}{m+n}}, \\sqrt{\\frac{6}{m+n}}\\right)\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "- $W_{ij}$ = weight connecting input $i$ to output $j$\n",
        "- $U(a, b)$ = uniform distribution between $a$ and $b$\n",
        "- $m$ = number of input dimensions\n",
        "- $n$ = number of output dimensions\n",
        "\n",
        "**For:** Linear networks, balanced activation and gradient variance\n",
        "\n",
        "---\n",
        "\n",
        "#### He Initialization:\n",
        "\n",
        "$$\n",
        "W_{ij} \\sim U\\left(-\\sqrt{\\frac{6}{m}}, \\sqrt{\\frac{6}{m}}\\right)\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "- $W_{ij}$ = weight connecting input $i$ to output $j$\n",
        "- $U(a, b)$ = uniform distribution between $a$ and $b$\n",
        "- $m$ = number of input dimensions\n",
        "\n",
        "**For:** ReLU networks, accounts for activation function properties\n",
        "\n",
        "---\n",
        "\n",
        "#### Orthogonal Initialization:\n",
        "\n",
        "**Approach:** Initialize weights as orthogonal matrices\n",
        "\n",
        "**Benefits:**\n",
        "- Maintains gradient flow through deep networks\n",
        "- Preserves norm of activations during forward pass\n",
        "- Prevents vanishing/exploding gradients\n",
        "\n",
        "**Scaling:** Often needs gain factor adjustment based on activation function\n",
        "\n",
        "---\n",
        "\n",
        "### Bias Initialization\n",
        "\n",
        "**General Rule:** Usually set to small constants (0 or small positive values)\n",
        "\n",
        "#### Special Cases:\n",
        "- **Output Units:** Set to match desired output statistics\n",
        "- **ReLU Units:** Sometimes set to 0.1 to avoid initial saturation (dead neurons)\n",
        "- **Gates (LSTM):** Set forget gates to 1 to keep information flowing initially\n",
        "\n",
        "---\n",
        "\n",
        "## Simple Explanation\n",
        "\n",
        "Think of training a neural network like teaching a group of students. If they all start with wildly different skill levels (bad initialization), some will excel while others give up immediately. Good initialization gives everyone a fair starting point.\n",
        "\n",
        "### Why Initialization Matters:\n",
        "\n",
        "**Too Large:** Weights explode → activations become huge → gradients explode → training fails\n",
        "\n",
        "**Too Small:** Weights vanish → activations die out → gradients disappear → network learns nothing\n",
        "\n",
        "**Just Right:** Signals flow smoothly through the network, gradients stay reasonable, training succeeds\n",
        "\n",
        "### The Different Strategies:\n",
        "\n",
        "**Glorot (Xavier) Initialization** is like giving students study materials proportional to both how much they need to learn (inputs) and how much they need to teach (outputs). It balances information flow in both directions.\n",
        "\n",
        "**He Initialization** is optimized for ReLU networks. ReLU kills half the neurons (negative values → zero), so He initialization compensates by starting with slightly larger weights. It's like giving students extra resources knowing half will be filtered out.\n",
        "\n",
        "**Orthogonal Initialization** ensures that information doesn't get distorted as it passes through layers. Think of it like keeping the volume constant on a stereo system as music passes through multiple components—no amplification or dampening at each stage.\n",
        "\n",
        "### Real-World Analogy:\n",
        "\n",
        "Imagine a relay race:\n",
        "- **Bad initialization:** Runners start at random speeds (some sprinting, some walking) → chaos\n",
        "- **Good initialization:** Everyone starts at a sustainable pace → smooth handoffs → team finishes strong\n",
        "\n",
        "### Bias Initialization Tips:\n",
        "\n",
        "**Starting at zero** works for most cases—it's neutral and lets the network learn what it needs.\n",
        "\n",
        "**Small positive values for ReLU** (like 0.1) prevent \"dead neurons\" that never activate. It's like giving neurons a small head start so they don't get stuck at zero forever.\n",
        "\n",
        "**Forget gate bias = 1 in LSTMs** means \"remember everything initially.\" The network can learn to forget later, but starting with memory helps learning.\n",
        "\n",
        "### When to Use What:\n",
        "\n",
        "- **Sigmoid/Tanh activations:** Use Glorot initialization\n",
        "- **ReLU/LeakyReLU activations:** Use He initialization  \n",
        "- **Very deep networks (100+ layers):** Consider orthogonal initialization\n",
        "- **Recurrent networks (RNN/LSTM):** Orthogonal for recurrent weights, He/Glorot for input weights"
      ],
      "metadata": {
        "id": "41YWtCAdd5HY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KEY INSIGHTS TO REMEMBER\n",
        "\n",
        "**The Core Principle:** Optimization is about finding good (not perfect) parameters efficiently\n",
        "\n",
        "**The Big Secret:** For large networks, local minima aren't the main problem - it's about finding any good minimum quickly\n",
        "\n",
        "**The Modern Reality:**\n",
        "* Adam + BatchNorm + Learning Rate Scheduling covers 90% of deep learning needs\n",
        "* Good initialization often matters more than perfect optimization\n",
        "\n",
        "**The Expert Mindset:**\n",
        "* Start simple (SGD/Momentum)\n",
        "* Add complexity only when needed\n",
        "* Monitor gradients, not just loss\n",
        "* Use adaptive methods for convenience, not perfection\n",
        "\n",
        "# OPTIMIZATION VS REGULARIZATION CONNECTION\n",
        "\n",
        "**Remember:** These two chapters are deeply connected:\n",
        "* Optimization: How to find good parameters\n",
        "* Regularization: How to make sure those parameters generalize\n",
        "* Both crucial: You need both for successful deep learning\n",
        "\n",
        "**Common Combinations:**\n",
        "* SGD + L2 Regularization (classic combination)\n",
        "* Adam + Dropout (modern deep learning)\n",
        "* Any Optimizer + Early Stopping (automatic regularization)"
      ],
      "metadata": {
        "id": "1cT1AF_-fY3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IB136DCzfXm7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}