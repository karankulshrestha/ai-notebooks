{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzwSvnfD9WpczCMWGW0u9Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karankulshrestha/ai-notebooks/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "y_h2HBhkQz_O"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(input, derivative=False):\n",
        "  if derivative:\n",
        "    return input * (1 - input)\n",
        "  return 1 / (1 + np.exp(-np.clip(input, -500, 500)))\n",
        "\n",
        "\n",
        "def tanh(input, derivative=False):\n",
        "  if derivative:\n",
        "    return 1 - input ** 2\n",
        "  return np.tanh(input)\n",
        "\n",
        "\n",
        "def softmax(x): # output activation\n",
        "  exp_x = np.exp(x - np.max(x))\n",
        "  return exp_x / np.sum(exp_x, axis=0, keepdims=True)"
      ],
      "metadata": {
        "id": "R5bEcpQ9ZhIp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM:\n",
        "  def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    # weights for forget gate\n",
        "    self.Wf = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
        "    self.bf = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # weights for input gate\n",
        "    self.Wi = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
        "    self.bi = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # weights for candidate cell state\n",
        "    self.Wc = np.random.randn(hidden_size, input_size + hidden_size) * 0.01 # Changed from 0.0 to 0.01\n",
        "    self.bc = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # weights for output gate\n",
        "    self.Wo = np.random.randn(hidden_size, input_size + hidden_size) * 0.01 # Corrected dimensions\n",
        "    self.bo = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # Initialize weights for final output\n",
        "    self.Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "    self.by = np.zeros((output_size, 1))\n",
        "\n",
        "  def forward(self, x, h_prev, c_prev):\n",
        "    \"\"\"\n",
        "    Forward pass for one time step\n",
        "    x: input at current time step (input_size, 1)\n",
        "    h_prev: hidden state from previous time step (hidden_size, 1)\n",
        "    c_prev: cell state from previous time step (hidden_size, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # concatenate the input and prev hidden state\n",
        "    concat = np.vstack((h_prev, x))\n",
        "\n",
        "    # forget gate\n",
        "    f = sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
        "\n",
        "    # input gate\n",
        "    i = sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
        "\n",
        "    # candidate cell state\n",
        "    c_tilde = tanh(np.dot(self.Wc, concat) + self.bc)\n",
        "\n",
        "    # update cell state\n",
        "    c = f * c_prev + i * c_tilde\n",
        "\n",
        "    # output gate\n",
        "    o = sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
        "\n",
        "    # update hidden state\n",
        "    h = o * tanh(c)\n",
        "\n",
        "    # Final output (logits)\n",
        "    y_logits = np.dot(self.Wy, h) + self.by\n",
        "\n",
        "    y = softmax(y_logits)\n",
        "\n",
        "    # store values for backward pass\n",
        "    cache = {\n",
        "        'x': x, 'h_prev': h_prev, 'c_prev': c_prev,\n",
        "        'concat': concat, 'f': f, 'i': i, 'c_tilde': c_tilde,\n",
        "        'c':c, 'o': o, 'h': h, 'y': y\n",
        "    }\n",
        "\n",
        "    return y, h, c, cache\n",
        "\n",
        "\n",
        "  def backward(self, dy, dh_next, dc_next, cache):\n",
        "      \"\"\"\n",
        "        Backward pass for one time step\n",
        "        dy: gradient of loss with respect to output\n",
        "        dh_next: gradient from next time step\n",
        "        dc_next: gradient of cell state from next time step\n",
        "      \"\"\"\n",
        "      x = cache['x']\n",
        "      h_prev = cache['h_prev']\n",
        "      c_prev = cache['c_prev']\n",
        "      concat = cache['concat']\n",
        "      f = cache['f']\n",
        "      i = cache['i']\n",
        "      c_tilde = cache['c_tilde']\n",
        "      c = cache['c']\n",
        "      o = cache['o']\n",
        "      h = cache['h']\n",
        "      y = cache['y']\n",
        "\n",
        "      # gradients of output layer\n",
        "      dWy = np.dot(dy, h.T)\n",
        "      dby = dy\n",
        "      dh = np.dot(self.Wy.T, dy) + dh_next\n",
        "\n",
        "      # gradient of output gate\n",
        "      do = dh * tanh(c)\n",
        "      do_input = sigmoid(o, derivative=True) * do\n",
        "\n",
        "      dWo = np.dot(do, concat.T)\n",
        "      dbo = do_input\n",
        "\n",
        "      # Gradient of cell state\n",
        "      dc = dh * o * tanh(tanh(c), derivative=True) + dc_next\n",
        "\n",
        "      # Gradient of candidate cell state\n",
        "      dc_tilde = dc * i\n",
        "      dc_tilde_input = tanh(c_tilde, derivative=True) * dc_tilde\n",
        "\n",
        "      dWc = np.dot(dc_tilde_input, concat.T)\n",
        "      dbc = dc_tilde_input\n",
        "\n",
        "      # Gradient of input gate\n",
        "      di = dc * c_tilde\n",
        "      di_input = sigmoid(i, derivative=True) * di\n",
        "\n",
        "      dWi = np.dot(di_input, concat.T)\n",
        "      dbi = di_input\n",
        "\n",
        "      # Gradient of forget gate\n",
        "      df = dc * c_prev\n",
        "      df_input = sigmoid(f, derivative=True) * df\n",
        "\n",
        "      dWf = np.dot(df_input, concat.T)\n",
        "      dbf = df_input\n",
        "\n",
        "      # sum of the gradient flow back through each gate\n",
        "      dconcat = (np.dot(self.Wf.T, df_input) +\n",
        "                  np.dot(self.Wi.T, di_input) +\n",
        "                  np.dot(self.Wc.T, dc_tilde_input) +\n",
        "                  np.dot(self.Wo.T, do_input))\n",
        "\n",
        "      # Split gradient for input and hidden_prev\n",
        "      dh_prev = dconcat[:self.hidden_size, :]\n",
        "      dx = dconcat[self.hidden_size:, :]\n",
        "\n",
        "      # Gradient for previous cell state\n",
        "      dc_prev = dc * f\n",
        "\n",
        "      gradients = {\n",
        "            'dWf': dWf, 'dbf': dbf,\n",
        "            'dWi': dWi, 'dbi': dbi,\n",
        "            'dWc': dWc, 'dbc': dbc,\n",
        "            'dWo': dWo, 'dbo': dbo,\n",
        "            'dWy': dWy, 'dby': dby\n",
        "        }\n",
        "\n",
        "      return dx, dh_prev, dc_prev, gradients\n",
        "\n",
        "  def train(self, X, Y, epochs=100, seq_length=25):\n",
        "     \"\"\"\n",
        "        Train the LSTM with sequence batching\n",
        "        X: input sequence of one-hot encoded characters (seq_length, vocab_size)\n",
        "        Y: target sequence of one-hot encoded characters (seq_length, vocab_size)\n",
        "        seq_length: length of subsequences for backpropagation through time\n",
        "     \"\"\"\n",
        "     n_sequences = len(X) // seq_length\n",
        "\n",
        "     for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for seq_idx in range(n_sequences):\n",
        "          # Get Subsequence\n",
        "          start_idx = seq_idx * seq_length\n",
        "          end_idx = start_idx + seq_length\n",
        "\n",
        "          # Initialize the hidden and cell states for this subsequence\n",
        "          h = np.zeros((self.hidden_size, 1))\n",
        "          c = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "          caches = []\n",
        "          loss = 0\n",
        "\n",
        "          # Forward pass through subsequence\n",
        "          for t in range(start_idx, min(end_idx, len(X))):\n",
        "            x = X[t].reshape(-1, 1)\n",
        "            y_target = Y[t].reshape(-1, 1) # convert it into column vector\n",
        "\n",
        "            y_pred, h, c, cache = self.forward(x, h, c)\n",
        "            caches.append(cache)\n",
        "\n",
        "            # Calculate loss (cross-entropy)\n",
        "            loss += -np.sum(y_target * np.log(y_pred + 1e-8))\n",
        "\n",
        "          # Backward pass for this subsequence\n",
        "          dh_next = np.zeros((self.hidden_size, 1))\n",
        "          dc_next = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "          # Accumulate gradients\n",
        "          grads = {\n",
        "              'dWf': np.zeros_like(self.Wf), 'dbf': np.zeros_like(self.bf),\n",
        "              'dWi': np.zeros_like(self.Wi), 'dbi': np.zeros_like(self.bi),\n",
        "              'dWc': np.zeros_like(self.Wc), 'dbc': np.zeros_like(self.bc),\n",
        "              'dWo': np.zeros_like(self.Wo), 'dbo': np.zeros_like(self.bo),\n",
        "              'dWy': np.zeros_like(self.Wy), 'dby': np.zeros_like(self.by)\n",
        "          }\n",
        "\n",
        "          for t in reversed(range(len(caches))):\n",
        "            y_target = Y[start_idx + t].reshape(-1, 1) # fetches the correct answer\n",
        "            y_pred = caches[t]['y'] # fetches the prediction answer\n",
        "\n",
        "            # Gradient of loss (cross-entropy with softmax)\n",
        "            dy = y_pred - y_target\n",
        "\n",
        "            dx, dh_next, dc_next, step_grads = self.backward(dy, dh_next, dc_next, caches[t])\n",
        "\n",
        "            # Accumulate gradients\n",
        "            for key in grads.keys():\n",
        "              grads[key] += step_grads[key]\n",
        "\n",
        "\n",
        "          # Clip gradients to prevent exploding gradients\n",
        "          for key in grads.keys():\n",
        "              grads[key] = np.clip(grads[key], -5, 5)\n",
        "\n",
        "\n",
        "\n",
        "          # Update weights after each subsequence\n",
        "          self.Wf -= self.learning_rate * grads['dWf']\n",
        "          self.bf -= self.learning_rate * grads['dbf']\n",
        "          self.Wi -= self.learning_rate * grads['dWi']\n",
        "          self.bi -= self.learning_rate * grads['dbi']\n",
        "          self.Wc -= self.learning_rate * grads['dWc']\n",
        "          self.bc -= self.learning_rate * grads['dbc']\n",
        "          self.Wo -= self.learning_rate * grads['dWo']\n",
        "          self.bo -= self.learning_rate * grads['dbo']\n",
        "          self.Wy -= self.learning_rate * grads['dWy']\n",
        "          self.by -= self.learning_rate * grads['dby']\n",
        "\n",
        "          total_loss += loss\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "              avg_loss = total_loss / (n_sequences * seq_length) if n_sequences > 0 else 0\n",
        "              print(f\"Epoch {epoch}, Loss: {avg_loss:.6f}\")\n",
        "\n",
        "\n",
        "  def generate_text(self, seed_char_idx, char_to_idx, idx_to_char, num_chars=50, temperature=1.0):\n",
        "      \"\"\"\n",
        "      Generate text starting from a seed character\n",
        "      temperature: controls randomness (higher = more random, lower = more deterministic)\n",
        "      \"\"\"\n",
        "      h = np.zeros((self.hidden_size, 1))\n",
        "      c = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "      generated = [idx_to_char[seed_char_idx]]\n",
        "      current_idx = seed_char_idx\n",
        "\n",
        "      for _ in range(num_chars):\n",
        "          # Create one-hot encoding\n",
        "          x = np.zeros((self.output_size, 1))\n",
        "          x[current_idx] = 1\n",
        "\n",
        "          # Forward pass\n",
        "          y_pred, h, c, _ = self.forward(x, h, c)\n",
        "\n",
        "          # Apply temperature and sample\n",
        "          y_pred = y_pred.flatten()\n",
        "          y_pred = np.log(y_pred + 1e-8) / temperature\n",
        "          y_pred = np.exp(y_pred) / np.sum(np.exp(y_pred))\n",
        "\n",
        "          # Sample next character based on probability distribution\n",
        "          current_idx = np.random.choice(len(y_pred), p=y_pred)\n",
        "          generated.append(idx_to_char[current_idx])\n",
        "\n",
        "      return ''.join(generated)"
      ],
      "metadata": {
        "id": "H7Ov7sYaaaq9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_char_data(text):\n",
        "\n",
        "  unique_chars = sorted(set(text))\n",
        "  vocab_size = len(unique_chars)\n",
        "\n",
        "  # Create character to index and index to character mappings\n",
        "  char_to_idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
        "  idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "\n",
        "  # convert chars to indices\n",
        "  char_indices = [char_to_idx[char] for char in text]\n",
        "\n",
        "  return char_indices, char_to_idx, idx_to_char, vocab_size\n",
        "\n",
        "\n",
        "def create_sequences(char_indices, vocab_size):\n",
        "    \"\"\"\n",
        "    Create input-output sequences for training\n",
        "    X: current character, Y: next character\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    for i in range(len(char_indices) - 1):\n",
        "        # Create one-hot encoding for input\n",
        "        x_onehot = np.zeros(vocab_size)\n",
        "        x_onehot[char_indices[i]] = 1\n",
        "        X.append(x_onehot)\n",
        "\n",
        "        # Create one-hot encoding for target\n",
        "        y_onehot = np.zeros(vocab_size)\n",
        "        y_onehot[char_indices[i + 1]] = 1\n",
        "        Y.append(y_onehot)\n",
        "\n",
        "    return np.array(X), np.array(Y)"
      ],
      "metadata": {
        "id": "ODU85z59_juD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "print(f\"Training on {len(text)} characters from Shakespeare\")\n",
        "print(f\"Preview: {text[:100]}...\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMncXMs-AooJ",
        "outputId": "92273744-a304-4678-e5da-060e0846a38e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 7280 characters from Shakespeare\n",
            "Preview: First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_indices, char_to_idx, idx_to_char, vocab_size = prepare_char_data(text)\n",
        "\n",
        "X, Y = create_sequences(char_indices, vocab_size)\n",
        "\n",
        "print(f\"Vocab size: {vocab_size}, Sequences: {len(X)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDLwLinDA64V",
        "outputId": "e0b4060c-d31a-4014-ad8e-4343179b593f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 56, Sequences: 7279\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = LSTM(vocab_size, hidden_size=256, output_size=vocab_size, learning_rate=0.005)\n",
        "print(\"Training (this may take a while)...\\n\")\n",
        "lstm.train(X, Y, epochs=140, seq_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPmA_iKiBFQu",
        "outputId": "b6e7cdaa-1e4b-4a7e-fe29-b302bd4f9389"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training (this may take a while)...\n",
            "\n",
            "Epoch 0, Loss: 3.601581\n",
            "Epoch 10, Loss: 2.972125\n",
            "Epoch 20, Loss: 2.419098\n",
            "Epoch 30, Loss: 2.083551\n",
            "Epoch 40, Loss: 1.855463\n",
            "Epoch 50, Loss: 1.614191\n",
            "Epoch 60, Loss: 1.403803\n",
            "Epoch 70, Loss: 1.153284\n",
            "Epoch 80, Loss: 0.961250\n",
            "Epoch 90, Loss: 0.738477\n",
            "Epoch 100, Loss: 0.553191\n",
            "Epoch 110, Loss: 0.410282\n",
            "Epoch 120, Loss: 0.200406\n",
            "Epoch 130, Loss: 0.157140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Generated text (temperature=0.8):\\n\")\n",
        "text = lstm.generate_text(char_to_idx['F'], char_to_idx, idx_to_char, num_chars=100, temperature=0.1)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMM3D5s_Cjcy",
        "outputId": "bfcb2e40-fc02-4343-98ed-f08e5b1d8f10"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Generated text (temperature=0.8):\n",
            "\n",
            "First Cithzens\n",
            "One tha goon'st the beall and their counts\n",
            "The bell, and we'll have cornted it plors, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9-Fa0DB9LgE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}